{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, ParameterGrid, train_test_split\n",
    "\n",
    "import utilities\n",
    "import visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The dataset consists of handwritten digits (0-9). Both training datasets conteins 7330 labeled patterns.\n",
    "- _\"pendigits_tr.txt\"_ conteins 16-dimensional patterns (x,y coordinates of eigth point equidistant after normalization and resampling);\n",
    "- _\"pendigits_tr_Pca_K2.txt\"_ is a two-dimensional version obtained by PCA, to visualize the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the dataset to visualize (16 or 2 features):\n",
    "dataset_path = 'DBs/PenDigits/pendigits_tr.txt'\n",
    "feature_count = 16\n",
    "\n",
    "#dataset_path = 'DBs/PenDigits/pendigits_tr_Pca_K2.txt'\n",
    "#feature_count = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape dataset: (7330, 16)\n",
      "Shape labels: (7330,)\n"
     ]
    }
   ],
   "source": [
    "# Loading of dataset:\n",
    "dataset_patterns, dataset_labels = utilities.load_labeled_dataset_from_txt(dataset_path, feature_count)\n",
    "print('Shape dataset:', dataset_patterns.shape)\n",
    "print('Shape labels:', dataset_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split the dataset in training set (60%) and validation (40%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape training set: (4398, 2)\n",
      "Shape validation set: (2932, 2)\n"
     ]
    }
   ],
   "source": [
    "train_x, validation_x, train_y, validation_y = train_test_split(dataset_patterns, dataset_labels, test_size=0.40)\n",
    "print('Shape training set:', train_x.shape)\n",
    "print('Shape validation set:', validation_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search and Cross-Validation\n",
    "To choose the type of classifier to use and set the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clf = KNeighborsClassifier()\n",
    "clf = SVC()\n",
    "\n",
    "# Creation of hyperparameters grid:\n",
    "param_grid = [\n",
    "   {'kernel': ['rbf'], 'C': [9,9.5], 'gamma': [1/9000,1/10000]}\n",
    "   # {'kernel': ['linear'], 'C': [0.05,0.5]}\n",
    "]\n",
    "\n",
    "# Number of fold for Cross-validation:\n",
    "n_folds = 5\n",
    "\n",
    "# Creation of a GridSearchCV object:\n",
    "grid_search_cv = GridSearchCV(clf, param_grid, cv=n_folds)\n",
    "\n",
    "# Research of hyperparameters:\n",
    "grid_search_cv.fit(dataset_patterns, dataset_labels)\n",
    "\n",
    "# Results:\n",
    "print('Combination of parameters:\\n', grid_search_cv.cv_results_['params'])\n",
    "print('Average accuracy for each combination:\\n', grid_search_cv.cv_results_['mean_test_score'])\n",
    "print('Best combination:\\n', grid_search_cv.best_params_)\n",
    "print('Average accuracy for the best combination: %.4f' % grid_search_cv.best_score_)\n",
    "\n",
    "# 2D Visualization:\n",
    "if feature_count ==2:\n",
    "    visualization.show_2D_results(grid_search_cv.best_estimator_,\n",
    "                                 (dataset_patterns, dataset_labels, 'Data'),\n",
    "                                 figsize=(9, 8))\n",
    "    # Note: \"grid_search_cv.best_estimator_\" train the dataset with the best combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I can train the final model to predict the data of the test set, contained in the files _\"pendigits_te.txt\"_ and _\"pendigits_te_Pca_K2.txt\"_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions correctly saved.\n"
     ]
    }
   ],
   "source": [
    "# Loading of test set\n",
    "if feature_count == 16:\n",
    "    test_path = 'DBs/PenDigits/pendigits_te.txt'\n",
    "elif feature_count == 2:\n",
    "    test_path = 'DBs/PenDigits/pendigits_te_Pca_K2.txt'\n",
    "    \n",
    "test_x = utilities.load_unlabeled_dataset_from_txt(test_path, feature_count)\n",
    "\n",
    "# Creation of the classifier:\n",
    "clf = SVC(kernel = 'rbf', C = 9.5, gamma = 1/9000)\n",
    "\n",
    "clf.fit(dataset_patterns, dataset_labels)\n",
    "\n",
    "# Compute the predictions:\n",
    "predictions = clf.predict(test_x)\n",
    "\n",
    "# Saving the predictions:\n",
    "with open(\"Predictions.txt\", \"w\") as f:\n",
    "    for prediction in predictions:\n",
    "        f.write(str(int(prediction)) + '\\n')\n",
    "print('Predictions correctly saved.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
